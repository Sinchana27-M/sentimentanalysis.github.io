{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "electronic-ceramic",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sinch\\Downloads\\Sentiment_Analysis_of_Covid19_Tweets_1 (2).ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sinch/Downloads/Sentiment_Analysis_of_Covid19_Tweets_1%20%282%29.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m### EDA Pkgs\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sinch/Downloads/Sentiment_Analysis_of_Covid19_Tweets_1%20%282%29.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "### EDA Pkgs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Viz Pkg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-perth",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'covid19_tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15204\\1177784995.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"covid19_tweets.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'covid19_tweets.csv'"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "df = pd.read_csv(\"covid19_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatype\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-people",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Source/ Value Count/Distribution of the Sources\n",
    "df['source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source/ Value Count/Distribution of the Sources\n",
    "df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-ridge",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the top value_counts\n",
    "df['source'].value_counts().nlargest(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-angola",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the top value_counts\n",
    "df['source'].value_counts().nlargest(30).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-undergraduate",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the top value_counts\n",
    "plt.figure(figsize=(20,10))\n",
    "df['source'].value_counts().nlargest(30).plot(kind='bar')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-commission",
   "metadata": {},
   "source": [
    "#### Text Analysis of tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install neattext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Text Cleaning Package\n",
    "import neattext.functions as nfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-guard",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Methods/Attrib\n",
    "dir(nfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-synthesis",
   "metadata": {},
   "source": [
    "### Noise\n",
    "+ remove mentions/userhandles\n",
    "+ remove hashtags\n",
    "+ urls\n",
    "+ emojis\n",
    "+ special char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-tumor",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'].apply(nfx.extract_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['extracted_hashtags'] = df['text'].apply(nfx.extract_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-realtor",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['extracted_hashtags','hashtags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Text\n",
    "df['clean_tweet'] = df['text'].apply(nfx.remove_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-blair",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['text','clean_tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: nfx.remove_userhandles(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-native",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['text','clean_tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Text: Multiple WhiteSpaces\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(nfx.remove_multiple_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Text : Remove urls\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(nfx.remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Text: Punctuations\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(nfx.remove_puncts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['text','clean_tweet']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-thermal",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment_polarity = blob.sentiment.polarity\n",
    "    sentiment_subjectivity = blob.sentiment.subjectivity\n",
    "    if sentiment_polarity > 0:\n",
    "        sentiment_label = 'Positive'\n",
    "    elif sentiment_polarity < 0:\n",
    "        sentiment_label = 'Negative'\n",
    "    else:\n",
    "        sentiment_label = 'Neutral'\n",
    "    result = {'polarity':sentiment_polarity,\n",
    "              'subjectivity':sentiment_subjectivity,\n",
    "              'sentiment':sentiment_label}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text\n",
    "ex1 = df['clean_tweet'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(ex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_results'] = df['clean_tweet'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_results'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(df['sentiment_results'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(pd.json_normalize(df['sentiment_results']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-chest",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with seaborn\n",
    "sns.countplot(df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keyword Extraction\n",
    "+ For Positive and Negative Sentiment\n",
    "+ General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet = df[df['sentiment'] == 'Positive']['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_tweet = df[df['sentiment'] == 'Neutral']['clean_tweet']\n",
    "negative_tweet = df[df['sentiment'] == 'Negative']['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-chuck",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords and Convert to Tokens\n",
    "positive_tweet_list = positive_tweet.apply(nfx.remove_stopwords).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tweet_list = negative_tweet.apply(nfx.remove_stopwords).tolist()\n",
    "neutral_tweet_list = neutral_tweet.apply(nfx.remove_stopwords).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-helen",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive_tweet_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "for line in positive_tweet_list:\n",
    "#     print(line)\n",
    "    for token in line.split():\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokens = [token for line in positive_tweet_list  for token in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tokens = [token for line in negative_tweet_list  for token in line.split()]\n",
    "neut_tokens = [token for line in neutral_tweet_list  for token in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-sunglasses",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Most Commonest Keywords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(docx,num=30):\n",
    "    word_tokens = Counter(docx)\n",
    "    most_common = word_tokens.most_common(num)\n",
    "    result = dict(most_common)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-negative",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_tokens(pos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_pos_words = get_tokens(pos_tokens)\n",
    "most_common_neg_words = get_tokens(neg_tokens)\n",
    "most_common_neut_words = get_tokens(neut_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with seaborn\n",
    "neg_df = pd.DataFrame(most_common_neg_words.items(),columns=['words','scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-beads",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "neg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(x='words',y='scores',data=neg_df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with seaborn\n",
    "pos_df = pd.DataFrame(most_common_pos_words.items(),columns=['words','scores'])\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(x='words',y='scores',data=pos_df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with seaborn\n",
    "neut_df = pd.DataFrame(most_common_neut_words.items(),columns=['words','scores'])\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(x='words',y='scores',data=neut_df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word Cloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(docx):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    mywordcloud = WordCloud().generate(docx)\n",
    "    plt.imshow(mywordcloud,interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_docx = ' '.join(pos_tokens)\n",
    "neg_docx = ' '.join(neg_tokens)\n",
    "neu_docx = ' '.join(neut_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(pos_docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(neg_docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(neu_docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(pos_docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-python",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-building",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
